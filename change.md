
### 核心设计思路：构建“视觉-文献”统一知识图谱

目前的系统仅从 `context.txt`（文献）中抽取知识构建图谱。修改的核心在于：**在进入推理循环前，先用大模型把图片“翻译”成详细的结构化描述，并将其视为一份“视觉文档”，利用现有的图谱构建能力从中抽取“视觉知识点”，最后与文献知识点合并。** 这样，图模式（Graph Mode）在采样路径时，就会自然地在“文献知识”和“视觉特征”之间跳转。

---

### 详细实施步骤

#### 第一步：新增“视觉理解与知识转化”模块

需要在 `pipeline/` 目录下创建一个新的模块文件（例如命名为 `pipeline_vision_knowledge.py`），负责以下两个核心功能：

1. **生成深度视觉描述** ：

* 定义一个函数，调用视觉模型（如 `MODEL_STAGE_1` 或更强的 `MODEL_SOLVE_STRONG`）。
* 使用专门的提示词（Prompt），要求模型不仅仅是生成标题，而是详细描述图片中的**实体对象、空间关系、文字读数、图表趋势、颜色状态**等细节。
* **关键点** ：提示词应要求模型输出客观陈述，便于后续提取“实体-关系-实体”的知识链。

1. **提取视觉知识链（Edges）** ：

* 复用 `graph/pipeline_graph.py` 中现有的 **知识链抽取逻辑** （即 `extract_edges_from_context` 函数）。
* 将上一步生成的“深度视觉描述”作为输入文本，通过文本模型抽取出结构化的知识边（Knowledge Edges）。
* **打标区分** ：在生成的 Edge 对象中增加一个标记（如 `source_type="image"`），以便后续区分这条知识是来自文献还是图片。

#### 第二步：修改 Episode 编排逻辑

需要修改 `pipeline/pipeline_episode.py` 中的 `run_episode` 函数，使其成为知识融合的入口：

1. **前置执行视觉理解** ：

* 在调用 `generate_steps` 之前，先调用第一步中新增的模块，获取**视觉描述文本**和 **视觉知识边列表** 。
* 建议将这一步结果缓存，避免重复调用视觉模型。

1. **参数透传** ：

* 将获取到的“视觉知识边列表”作为新参数传递给 `generate_steps` 函数。

#### 第三步：修改图模式生成逻辑

这是修改的重点，位于 `steps/graph_mode.py` 文件中，目的是实现混合采样：

1. **构建统一实体池** ：

* 在 `generate_steps_graph_mode` 函数内部，接收传入的“视觉知识边”。
* 获取原有的文献知识边（`build_knowledge_edges_cached` 的结果）。
* **合并操作** ：将“视觉知识边”与“文献知识边”合并为一个总的边列表（`all_edges`）。此时，系统就拥有了一个包含“文本概念”和“视觉元素”的统一图谱。

1. **路径采样策略优化** ：

* 利用现有的 `sample_path` 函数对 `all_edges` 进行采样。
* 由于图谱已合并，随机游走算法（Random Walk）或广度优先搜索（BFS）会自动产生跨模态的路径。例如：路径可能从文献中的“原理公式”跳转到视觉描述中的“仪表盘读数”。
* **增强约束（可选）** ：可以在采样逻辑中增加权重，强制要求采样的路径必须包含至少一条来自“视觉源”的边，确保每一轮 Step 都在利用图片知识。

1. **Prompt 动态适配** ：

* 在构建 Step 的 Prompt（如 `build_graph_1hop_step_prompt`）时，需要根据当前采样到的边是来自“文献”还是“图片”来调整提示词前缀。
* 如果是视觉边，提示词中应明确：“根据对图片的视觉分析（Visual Analysis）...”。
* 如果是文献边，提示词保持原样：“根据参考信息（Reference）...”。

#### 第四步：修改 Step 提示词模板

需要微调 `prompts/steps.py` 中的模板，使其能消化视觉描述类型的知识：

1. **新增视觉上下文输入** ：

* 除了现有的 `context` 和 `fact_hint`，在 Prompt 模板中增加一个字段，专门展示“图片详细文本描述”的摘要。
* 这可以作为除了 `image_path`（直接看图）之外的辅助信息，帮助模型更好地理解那些模糊的视觉细节。

1. **知识来源标注** ：

* 在 `fact_hint`（事实提示）部分，明确标注当前知识点的来源。例如：`[来源: 文献 L10-12]` 或 `[来源: 图片视觉分析]`。这有助于模型区分它是需要去“读文档”还是去“看图核实”。
